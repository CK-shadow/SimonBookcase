---
title: 电商系统常见问题
date: 2022-03-28 22:37:38
tags: 电商
categories: 后端架构
top_img: https://cdn.jsdelivr.net/gh/CK-shadow/picx-image@master/images/后端架构/电商系统中的常见问题/F100029622.kg8c53xfzvk.webp
cover: https://cdn.jsdelivr.net/gh/CK-shadow/picx-image@master/images/后端架构/电商系统中的常见问题/F100029622.kg8c53xfzvk.webp
---



## 电商系统常见问题



#### 如何避免重复下单



无论是用户重复点击、网络重传还是一些框架的自动重试功能，都会导致用户重复下单。而要避免用户重复下单，最简单的方式就是保证订单服务接口的幂等性，即在下单的时候传入一个订单号，如果这个订单号已经存在的话，就返回当前订单信息、返回异常信息或者不做处理



------

#### 电商系统如何存储数据



在数据量较少的时候，直接使用传统数据库存储即可，但是数据量变大，数据信息变得复杂的时候，就需要针对不同的数据特点采取不同的存储方式

* 商品的基本信息存储在传统数据库即可，对于一些读多写少的数据，还可以加上缓存
* 对于字段不固定的数据，比如商品参数，可以使用 MongoDB 来存储
* 图片和视频存储在对象存储中，数据库中只用存储对应的路径或 id 即可
* 将变动较小的页面静态化，并使用 CDN 加速



------

#### 购物车数据应该怎么存储



一个常见的购物车表一般主要有 4 个字段，购物车 id，商品 id，商品数量，是否选中

如果字段比较固定的话，可以将购物车信息存在 Redis 中，牺牲一点安全性带来性能的大幅提升（Redis 数据可能会丢失）。但是考虑到业务的变化，以及 Redis 不支持事务等操作，而且有些商品的购物车信息是会占据库存的，因此对安全性要求较高。所以比较支持购物车信息存在 MySQL 之中



------

#### 如何保证账务系统数据的准确性



在账务系统中，用户余额是最重要的一个数据，但是可能会因为网络原因、人为修改或其它原因造成这个数据不正确，这时候就需要通过交易流水来重新统计用户的余额，因此交易流水的记录是非常有必要的



在设计账户流水时，有几个重要的原则必须遵守，最好是用技术手段加以限制

1. 流水记录只能新增，一旦记录成功不允许修改和删除。即使是由于正当原因需要取消一笔已经完成的交易，也不应该去删除交易流水。正确的做法是再记录一笔“取消交易”的流水
2. 流水号必须是递增的，我们需要用流水号来确定交易的先后顺序



我们需要在实现交易功能的时候，同时记录流水并修改余额，并且要尽可能保证，在任何情况下，记录流水和修改余额这两个操作，要么都成功，要么都失败。这时候就需要通过数据库的事务来保证该流程的实现



------

#### 电商系统中的分布式事务



分布式事务的解决方案有很多，比如：2PC、3PC、TCC、Saga 和本地消息表等等。这些方法，它的强项和弱项都不一样，适用的场景也不一样。这里面，2PC 和本地消息表这两种分布式事务的解决方案，比较贴近于我们日常开发的业务系统



2PC：订单与优惠券的数据一致性问题

比如我们在使用优惠券购买商品时，需要同时更新订单系统和优惠券系统的数据，并且要求数据保持一致。2PC 引入了一个事务协调者的角色，来协调订单系统和促销系统，由协调者向外暴露一个使用优惠券支付的接口。外部服务调用这个接口之后，协调者分别向订单系统和促销系统发出一个准备命令，订单系统和促销系统开启事务并更新数据，但不提交事务，然后向协调者响应准备完成，在协调者收到两个准备完成的响应之后，再分别发出执行命令，订单系统和促销系统提交事务

![image-20220321002309571](https://cdn.jsdelivr.net/gh/CK-shadow/picx-image@master/images/后端架构/电商系统中的常见问题/图片.7civ3ndqvvcw.webp)

在准备阶段，如果任何一步出现错误或者是超时，协调者就会给两个系统发送“回滚事务”请求。但在进入提交阶段之后，就没有回头路了。如果发生网络传输失败的情况，需要反复重试，直到提交成功为止。如果这个阶段发生宕机，包括两个数据库宕机或者订单服务、促销服务所在的节点宕机，还是有可能出现订单库完成了提交，但促销库因为宕机自动回滚，导致数据不一致的情况。但是，因为提交的过程非常简单，执行很快，出现这种情况的概率非常小

2PC 比较适合那些对数据一致性要求比较高的场景。2PC 也有很明显的缺陷，整个事务的执行过程需要阻塞服务端的线程和数据库的会话，所以，2PC 在并发场景下的性能不会很高。并且，协调者是一个单点，一旦过程中协调者宕机，就会导致订单库或者促销库的事务会话一直卡在等待提交阶段，直到事务超时自动回滚。所以，只有在需要强一致、并且并发量不大的场景下，才考虑使用 2PC



本地消息表：订单与购物车的数据一致性问题

一般情况下，在我们购买商品后，需要清空该商品在购物车中的数据。但是它对数据的一致性要求没有那么高，购物车晚一会儿清空也是可以的，只需要最终达到目的即可。这样的话，我们可以在下单之后记录一条消息，然后返回成功，再慢慢去消费这条消息，直到成功为止。记录消息的方式可以日志、文件、数据库或其它方式，这个记录消息的位置就称为本地消息表

本地消息表实现的是数据的最终一致性，实现起来比较简单，速度也更快。但是即使能接受数据最终一致，本地消息表也不是什么场景都可以适用的。它有一个前提条件就是，异步执行的那部分操作，不能有依赖的资源。比如说，我们下单的时候，除了要清空购物车以外，还需要锁定库存

库存系统锁定库存这个操作，虽然可以接受数据最终一致，但是，锁定库存这个操作是有一个前提的，这个前提是：库存中得有货。这种情况就不适合使用本地消息表，不然就会出现用户下单成功后，系统的异步任务去锁定库存的时候，因为缺货导致锁定失败。这样的情况就很难处理了



------

#### MySQL 读写分离，JDBC 如何支持



1. 纯手工方式：修改应用程序的 DAO 层代码，定义读写两个数据源，指定每一个数据库请求的数据源
2. 组件方式：也可以使用像 Sharding-JDBC 这种集成在应用中的第三方组件来实现，这些组件集成在你的应用程序内，代理应用程序的所有数据库请求，自动把请求路由到对应数据库实例上
3. 代理方式：在应用程序和数据库实例之间部署一组数据库代理实例，比如说 Atlas 或者 MaxScale。对应用程序来说，数据库代理把自己伪装成一个单节点的 MySQL 实例，应用程序的所有数据库请求被发送给代理，代理分离读写请求，然后转发给对应的数据库实例



------

#### MySQL 读写分离，数据不一致怎么办



1. 修改业务逻辑，更新数据完成后不立即查询或者返回用户结果，跳转至一个过渡页
2. 把更新数据和读数据放在同一个事务中，同一个事务中的查询操作也会被路由到主库



------

#### MySQL 主从同步的方式



1. 异步复制

   MySQL  主库在收到客户端提交事务的请求之后，会先写入  Binlog，然后再提交事务，更新存储引擎中的数据，事务提交完成后，给客户端返回操作成功的响应。同时，从库会有一个专门的复制线程，从主库接收  Binlog，然后把 Binlog 写到一个中继日志里面，再给主库返回复制成功的响应

2. 同步复制

   异步复制时，主库提交事务之后，就会给客户端返回响应；而同步复制时，主库在提交事务的时候，会等待数据复制到所有从库之后，再给客户端返回响应

3. 半同步复制

   半同步复制介于二者之间，事务线程不用等着所有的复制成功响应，只要一部分复制响应回来之后，就可以给客户端返回了



------

#### 海量数据怎么办



对于有时限性的数据，比如订单表，最简单的方式就是区分历史表，因为历史数据的操作是很少的

如果数据量很大，而且访问量随机的话，就需要分库分表了。数据量大，就分表；并发高，就分库。但是分库分表又会造成查询不够随意，所以分库分表一定是数据量和并发大到所有招数都不好使了，我们才拿出来的最后一招

常用三种分片算法，范围分片容易产生热点问题，但对查询更友好，适合适合并发量不大的场景；哈希分片比较容易把数据和查询均匀地分布到所有分片中；查表法更灵活，但性能稍差



------

#### Redis 集群方案



* 小规模的集群（几个到几十个节点）建议使用官方的 Redis  Cluster，在节点数量不多的情况下，各方面表现都不错。

* 再大一些规模的集群，可以考虑使用 twemproxy 或者 Codis  这类的基于代理的集群架构，虽然是开源方案，但是已经被很多公司在生产环境中验证过。

* 相比于代理方案，使用定制客户端的方案性能更好，很多大厂采用的都是类似的架构



------

#### 如何保证 Redis 和 MySQL 数据同步



在处理超大规模并发的场景时，由于并发请求的数量非常大，即使少量的缓存穿透，也有可能打死数据库引发雪崩效应。对于这种情况，我们可以缓存全量数据来彻底避免缓存穿透问题

对于缓存数据更新的方法，可以订阅数据更新的 MQ 消息来异步更新缓存，更通用的方法是，把缓存更新服务伪装成一个 MySQL 的从节点，订阅 MySQL 的 Binlog，通过 Binlog 来更新 Redis 缓存

需要特别注意的是，无论是用 MQ 还是  Canal 来异步更新缓存，对整个更新服务的数据可靠性和实时性要求都比较高，数据丢失或者更新慢了，都会造成 Redis 中的数据与 MySQL  中数据不同步。在把这套方案应用到生产环境中去的时候，需要考虑一旦出现不同步问题时的降级或补偿方案



------

#### MySQL 数据同步到 Redis，并发量太大造成同步较慢怎么办



对于海量数据，必须要按照查询方式选择数据库类型和数据的组织方式，才能达到理想的查询性能。这就需要把同一份数据，按照不同的业务需求，以不同的组织方式存放到各种异构数据库中。因为数据的来源大多都是在线交易系统的 MySQL 数据库，所以我们可以利用 MySQL 的 Binlog 来实现异构数据库之间的实时数据同步

为了能够支撑众多下游数据库实时同步的需求，可以通过 MQ 解耦上下游，Binlog 先发送到 MQ 中，下游各业务方可以消费 MQ 中的消息再写入各自的数据库

如果下游处理能力不能满足要求，可以增加 MQ 中的分区数量实现并发同步，但需要结合同步的业务数据特点，把具有因果关系的数据哈希到相同分区上，才能避免因为并发乱序而出现数据同步错误的问题



------

#### 如何不停机的安全切换数据库



设计在线切换数据库的技术方案，首先要保证安全性，确保每一个步骤一旦失败，都可以快速回滚。此外，还要确保迁移过程中不丢数据，这主要是依靠实时同步程序和对比补偿程序来实现



------

#### 埋点的海量数据如何存储



在互联网行业，点击流、监控和日志这几类数据，是海量数据中的海量数据。对于这类数据，一般的处理方式都是先存储再计算，计算结果保存到特定的数据库中，供业务系统查询

所以，对于海量原始数据的存储系统，我们要求的是超高的写入和读取性能，和近乎无限的容量，对于数据的查询能力要求不高。生产上，可以选择 Kafka 或者是 HDFS，Kafka 的优点是读写性能更好，单节点能支持更高的吞吐量。而 HDFS  则能提供真正无限的存储容量，并且对查询更友好

未来会有一些开源的流数据存储系统和时序数据库逐步成熟，并陆续应用到生产系统中去，你可以持续关注这些项目
